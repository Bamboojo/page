
<!DOCTYPE html>
<html>
<head>
<APM_DO_NOT_TOUCH>
<script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>
    <!-- 引入MathJax库 -->
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script type="text/javascript">
(function(){
window.Byl=!!window.Byl;try{(function(){(function(){var a=-1;a={A:++a,ec:"false"[a],a:++a,Ja:"false"[a],K:++a,$e:"[object Object]"[a],eb:(a[a]+"")[a],Ka:++a,cb:"true"[a],D:++a,M:++a,fc:"[object Object]"[a],o:++a,V:++a,tj:++a,sj:++a};try{a.Ia=(a.Ia=a+"")[a.M]+(a.xa=a.Ia[a.a])+(a.dc=(a.wa+"")[a.a])+(!a+"")[a.Ka]+(a.ya=a.Ia[a.o])+(a.wa="true"[a.a])+(a.gb="true"[a.K])+a.Ia[a.M]+a.ya+a.xa+a.wa,a.dc=a.wa+"true"[a.Ka]+a.ya+a.gb+a.wa+a.dc,a.wa=a.A[a.Ia][a.Ia],a.wa(a.wa(a.dc+'"\\'+a.a+a.M+a.a+a.ec+"\\"+a.D+a.A+"("+a.ya+"\\"+a.a+a.V+a.a+"\\"+a.a+
a.o+a.A+a.cb+a.xa+a.ec+"\\"+a.D+a.A+"\\"+a.a+a.o+a.V+"\\"+a.a+a.M+a.a+"\\"+a.a+a.M+a.o+a.eb+a.xa+"\\"+a.a+a.o+a.V+"['\\"+a.a+a.o+a.A+a.Ja+"\\"+a.a+a.V+a.a+"false"[a.K]+a.xa+a.Ja+a.eb+"']\\"+a.D+a.A+"===\\"+a.D+a.A+"'\\"+a.a+a.o+a.Ka+a.ya+"\\"+a.a+a.o+a.K+"\\"+a.a+a.M+a.a+"\\"+a.a+a.M+a.o+"\\"+a.a+a.D+a.V+"')\\"+a.D+a.A+"{\\"+a.a+a.K+"\\"+a.a+a.a+"\\"+a.a+a.o+a.o+a.Ja+"\\"+a.a+a.o+a.K+"\\"+a.D+a.A+a.cb+a.eb+"\\"+a.a+a.o+a.o+a.fc+"\\"+a.a+a.V+a.a+a.gb+"\\"+a.a+a.M+a.K+"\\"+a.a+a.M+a.Ka+"\\"+a.a+a.o+
a.A+"\\"+a.D+a.A+"=\\"+a.D+a.A+"\\"+a.a+a.o+a.V+"\\"+a.a+a.M+a.a+"\\"+a.a+a.M+a.o+a.eb+a.xa+"\\"+a.a+a.o+a.V+"['\\"+a.a+a.o+a.A+a.Ja+"\\"+a.a+a.V+a.a+"false"[a.K]+a.xa+a.Ja+a.eb+"'].\\"+a.a+a.o+a.K+a.cb+"\\"+a.a+a.o+a.A+"false"[a.K]+a.Ja+a.fc+a.cb+"(/.{"+a.a+","+a.D+"}/\\"+a.a+a.D+a.V+",\\"+a.D+a.A+a.ec+a.gb+"\\"+a.a+a.M+a.o+a.fc+a.ya+"\\"+a.a+a.M+a.a+a.xa+"\\"+a.a+a.M+a.o+"\\"+a.D+a.A+"(\\"+a.a+a.V+a.A+")\\"+a.D+a.A+"{\\"+a.a+a.K+"\\"+a.a+a.a+"\\"+a.a+a.a+"\\"+a.a+a.a+"\\"+a.a+a.o+a.K+a.cb+a.ya+
a.gb+"\\"+a.a+a.o+a.K+"\\"+a.a+a.M+a.o+"\\"+a.D+a.A+"(\\"+a.a+a.V+a.A+"\\"+a.D+a.A+"+\\"+a.D+a.A+"\\"+a.a+a.V+a.A+").\\"+a.a+a.o+a.Ka+a.gb+a.$e+"\\"+a.a+a.o+a.Ka+a.ya+"\\"+a.a+a.o+a.K+"("+a.K+",\\"+a.D+a.A+a.D+")\\"+a.a+a.K+"\\"+a.a+a.a+"\\"+a.a+a.a+"});\\"+a.a+a.K+"}\\"+a.a+a.K+'"')())()}catch(d){a%=5}})();var b=69;
try{var ba,ea,ma=c(618)?1:0,pa=c(678)?1:0,qa=c(653)?1:0,sa=c(301)?1:0,ua=c(881)?0:1;for(var za=(c(430),0);za<ea;++za)ma+=c(661)?2:1,pa+=c(579)?2:1,qa+=(c(398),2),sa+=c(602)?2:1,ua+=c(967)?1:3;ba=ma+pa+qa+sa+ua;window.fb===ba&&(window.fb=++ba)}catch(a){window.fb=ba}var e=!0;function f(a){var d=arguments.length,g=[];for(var h=1;h<d;h++)g[h-1]=arguments[h]-a;return String.fromCharCode.apply(String,g)}
function Aa(a){var d=59;a&&(document[f(d,177,164,174,164,157,164,167,164,175,180,142,175,156,175,160)]&&document[r(d,177,164,174,164,157,164,167,164,175,180,142,175,156,175,160)]!==f(d,177,164,174,164,157,167,160)||(e=!1));return e}function r(a){var d=arguments.length,g=[];for(var h=1;h<d;++h)g.push(arguments[h]-a);return String.fromCharCode.apply(String,g)}function Ca(){}Aa(window[Ca[f(b,179,166,178,170)]]===Ca);Aa(typeof ie9rgb4!==r(b,171,186,179,168,185,174,180,179));
Aa(RegExp("\x3c")[r(b,185,170,184,185)](function(){return"\x3c"})&!RegExp(t(42820,b))[t(1372136,b)](function(){return"'x3'+'d';"}));
var Ea=window[f(b,166,185,185,166,168,173,138,187,170,179,185)]||RegExp(f(b,178,180,167,174,193,166,179,169,183,180,174,169),t(-51,b))[t(1372136,b)](window["\x6e\x61vi\x67a\x74\x6f\x72"]["\x75\x73e\x72A\x67\x65\x6et"]),Fa=+new Date+(c(729)?555981:6E5),Ga,Ia,Ja,Ka=window[f(b,184,170,185,153,174,178,170,180,186,185)],La=Ea?c(853)?15657:3E4:c(907)?3104:6E3;
document[f(b,166,169,169,138,187,170,179,185,145,174,184,185,170,179,170,183)]&&document[r(b,166,169,169,138,187,170,179,185,145,174,184,185,170,179,170,183)](r(b,187,174,184,174,167,174,177,174,185,190,168,173,166,179,172,170),function(a){var d=49;document[r(d,167,154,164,154,147,154,157,154,165,170,132,165,146,165,150)]&&(document[r(d,167,154,164,154,147,154,157,154,165,170,132,165,146,165,150)]===t(1058781934,d)&&a[r(d,154,164,133,163,166,164,165,150,149)]?Ja=!0:document[r(d,167,154,164,154,147,
154,157,154,165,170,132,165,146,165,150)]===t(68616527617,d)&&(Ga=+new Date,Ja=!1,w()))});function t(a,d){a+=d;return a.toString(36)}function w(){if(!document[f(49,162,166,150,163,170,132,150,157,150,148,165,160,163)])return!0;var a=+new Date;if(a>Fa&&(c(955)?536984:6E5)>a-Ga)return Aa(!1);var d=Aa(Ia&&!Ja&&Ga+La<a);Ga=a;Ia||(Ia=!0,Ka(function(){Ia=!1},c(922)?0:1));return d}w();var Na=[c(256)?17795081:10477169,c(774)?2147483647:27611931586,c(201)?1558153217:1732073060];
function Oa(a){var d=75;a=typeof a===t(1743045601,d)?a:a[f(d,191,186,158,191,189,180,185,178)](c(797)?42:36);var g=window[a];if(!g||!g[r(d,191,186,158,191,189,180,185,178)])return;var h=""+g;window[a]=function(k,l){Ia=!1;return g(k,l)};window[a][r(d,191,186,158,191,189,180,185,178)]=function(){return h}}for(var Qa=(c(516),0);Qa<Na[t(1294399136,b)];++Qa)Oa(Na[Qa]);Aa(!1!==window[r(b,135,190,177)]);window.Ta=window.Ta||{};window.Ta.oc="08ae257f95194000f9b2e6e601d950dbc4209bf745eaa7c0aa496fc4379920ade4f4f40883639759e1ba712bcef222bac430b463ddfac3ca8c7055c6568a977403b82e0abecbacab";
function B(a){var d=+new Date;if(!document[r(49,162,166,150,163,170,132,150,157,150,148,165,160,163,114,157,157)]||d>Fa&&(c(240)?6E5:761817)>d-Ga)var g=Aa(!1);else g=Aa(Ia&&!Ja&&Ga+La<d),Ga=d,Ia||(Ia=!0,Ka(function(){Ia=!1},c(327)?1:0));return!(arguments[a]^g)}function c(a){return 697>a}(function(a){a||setTimeout(function(){var d=setTimeout(function(){},250);for(var g=0;g<=d;++g)clearTimeout(g)},500)})(!0);})();}catch(x){}finally{ie9rgb4=void(0);};function ie9rgb4(a,b){return a>>b>>0};

})();

</script>
</APM_DO_NOT_TOUCH>

<script type="text/javascript" src="/TSPD/082149a1b4ab2000d35501b417909f6722d0e2a3f6da28eaa8ea085bdd194111b76f5f6e048a57e2?type=9"></script>

  <meta charset="utf-8">
  <meta name="description" content="We first explain how self-supervised representations can be easily used to achieve state-of-the-art performance in commonly reported anomaly detection benchmarks. We then argue that tackling the next-generation of anomaly detection tasks requires new technical and conceptual improvements in representation learning.">
  <meta property="og:title" content="Controllable Unlearning for Image-to-Image
Generative Models via ε-Constrained Optimization"/>
  <meta property="og:description" content="We first explain how self-supervised representations can be easily used to achieve state-of-the-art performance in commonly reported anomaly detection benchmarks. We then argue that tackling the next-generation of anomaly detection tasks requires new technical and conceptual improvements in representation learning."/>
  <meta property="og:url" content="https://www.vision.huji.ac.il/ssrl_ad/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="/static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Anomaly Detection, Representation Learning, Self-Supervised Representation Learning, SSL, SSRL, ECCV, ECCV 2022, SSLWIN">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Controllable Unlearning for Image-to-Image
Generative Models via ε-Constrained Optimization</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Controllable Unlearning for Image-to-Image
Generative Models via ε-Constrained Optimization</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="#" style="text-decoration: none; color: inherit;">Xiaohua Feng</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=qZTMyzwAAAAJ" target="_blank">Chaochao Chen</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=v4e49qEAAAAJ" target="_blank">Yuyuan Li</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a href="#" style="text-decoration: none; color: inherit;">Li Zhang</a><sup>1</sup>,
              </span>
          
            </div>

           <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Zhejiang University</span>
                    &nbsp;
                   
                    <span class="author-block"><sup>2</sup>Hangzhou Dianzi University</span>
                    
                  </div>

           


            <!-- ArXiv abstract Link -->
            <span class="link-block">
              <a href="https://arxiv.org/abs/2408.01689"  target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="ai ai-arxiv"></i>
              </span>
              <span>arXiv</span>
            </a>
          </span>
        </div>
      </div>
    </div>
  </div>
</div>
</div>
</section>





<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          While generative models have made significant advancements in recent years, they
            also raise concerns such as privacy breaches and biases. Machine unlearning
            has emerged as a viable solution, aiming to remove specific training data, e.g.,
            containing private information and bias, from models. In this paper, we study the
            machine unlearning problem in Image-to-Image (I2I) generative models. Previous
            studies mainly treat it as a single objective optimization problem, offering a solitary
            solution, thereby neglecting the varied user expectations towards the trade-off
            between complete unlearning and model utility. To address this issue, we propose
            a controllable unlearning framework that uses a control coefficient ε to control
            the trade-off. We reformulate the I2I generative model unlearning problem into
            a ε-constrained optimization problem and solve it with a gradient-based method
            to find optimal solutions for unlearning boundaries. These boundaries define the
            valid range for the control coefficient. Within this range, every yielded solution is
            theoretically guaranteed with Pareto optimality. We also analyze the convergence
            rate of our framework under various control functions. Extensive experiments
            on two benchmark datasets across three mainstream I2I models demonstrate the
            effectiveness of our controllable unlearning framework.
         </p>
       </div>
     </div>
   </div>
 </div>
</section>
<!-- End paper abstract -->




<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Research background</h2>
          <div class="level-set has-text-justified">
            <p>
              In this paper, we focus on the unlearning problem in Image-to-Image (I2I) generative models,
where unlearning is defined by the model’s incapacity to reconstruct the full image from a partially
cropped one , as shown in Figure 1. Previous study frames machine unlearning in generative
models as a single-objective optimization problem, with the loss defined as a combination of performance on both the forget and retain sets. To address these challenges, we propose a controllable unlearning approach that provides a set
of Pareto optimal solutions to cater to varied user expectations. Users can select a solution based
on the degree of unlearning completeness through a simple control coefficient ε.
            </p>
          </div>
          <img src="static/images/image1.png" alt="Normal and Anomalous Representations" class="center-image"/>
 <figcaption class="image-caption"> Figure 1：On the left, the first and second rows represent the
forget set and the retain set, respectively. We first present the effect of unlearning in I2I generative
models, followed by a collection of controllable solutions, where ε is the control coefficient. On the
right, we demonstrate that for each ε, our solution is guaranteed with the Pareto optimality.</figcaption>
  </figure>        
</div>
      </div>
    </div>
  </div>
</section>



<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Pipeline of the controllable unlearning framework</h2>
          <div class="level-set has-text-justified">
            <p>
             
            </p>
          </div>
          <img src="static/images/image2.png" alt="Normal and Anomalous Representations" class="center-image"/>
 <figcaption class="image-caption">(a) shows the unlearning task of the I2I generative model which is framed as an $ \varepsilon $-constrained optimization problem. 
        (b) shows that the implementation of controllable unlearning unfolds in two phases: 
        i) initially identifying two boundary points of unlearning, necessitating a strict reduction in $ f_1(\theta) $ (or $ f_2(\theta) $) for optimality; 
        and ii) then locating the given $ \varepsilon $’s Pareto optimal point, with strict reduction in $ f_1(\theta) $ when $ f_1(\theta_t) > \varepsilon $ and permitting an increase when $ f_1(\theta_t) \leq \varepsilon $.</figcaption>
  </figure>        
</div>
      </div>
    </div>
  </div>
</section>



<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Main Experiments</h2>
          <div class="level-set has-text-justified">
            <p>
             We conduct experiments on the following two
large-scale datasets: i) ImageNet-1K , from which we randomly select 200 classes, designating
100 of these as the forget set and the remaining 100 as the retain set. Each class contains 150 images,
with 100 allocated for training and the remaining for validation; and ii) Places-365 , from which
we randomly select 100 classes, designating 50 of these as the forget set and the remaining 50 as
the retain set. Each class contains 5500 images, with 5000 allocated for training and the remaining
500 for validation. 
            </p>
          </div>
          <img src="static/images/image5.png" alt="Normal and Anomalous Representations" class="center-image"/>
 <figcaption class="image-caption">Generated images of cropping 50% at the center of the image on VQ-GAN. From left to
right, the images generated by baselines are presented. Our method results in the highest degree of
unlearning completeness while maintaining a minimal reduction in model utility.</figcaption>
  </figure> 
<h2 class="title is-3">Results of center cropping 50% of the images</h2> 
<div class="level-set has-text-justified">
            <p>
          ‘F’ and ‘R’ stand for the forget set and retain
set, respectively. Here, "Ours" refers to the boundary points of unlearning obtained in Phase I, that is,
the solution with the highest degree of unlearning completeness.
 </p>
          </div>
          <img src="static/images/image3.png" alt="Normal and Anomalous Representations" class="center-image"/>

  </figure>               
</div>
      </div>
    </div>
  </div>
</section>



<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">T-SNE analysis</h2>
          <div class="level-set has-text-justified">
            <p>
            We conduct a T-SNE analysis to further analyze our method’s effectiveness. Using our unlearned
model, we generate 50 images for both the retain set and the forget set. We then calculate the CLIP
embedding vectors for these images and their corresponding ground truth images.
            </p>
          </div>
          <img src="static/images/image6.png" alt="Normal and Anomalous Representations" class="center-image"/>
 <figcaption class="image-caption">T-SNE analysis between images generated by our method and ground truth images.</figcaption>
  </figure> 
<h2 class="title is-3">Results of center cropping 50% of the images under different unlearning completeness</h2> 
<div class="level-set has-text-justified">
           <p>
           
“Highest” and “Lowest” respectively represent the two boundary points of unlearning identified in
Phase I. ε is a coefficient used to control the unlearning completeness in Phase II.
            </p>
          </div>
          <img src="static/images/image4.png" alt="Normal and Anomalous Representations" class="center-image"/>
              
</div>
      </div>
    </div>
  </div>
</section>



<section class="hero is-small">

<div class="hero-body">
    <div class="container is-max-desktop">
     <h2 class="title is-3">More Generated Images</h2>
 <div class="level-set has-text-justified">
            <p>
           We conduct evaluations of image inpainting and
expansion tasks on VQ-GAN, image reconstruction tasks on MAE, and image inpainting tasks on the
diffusion model.
            </p>
<div id="results-carousel" class="carousel results-carousel">
     <div class="item">
        <center>
        <img src="static/images/image7.png" alt="Bounds for super-resolution"/>
        </center>
        <h2 class="subtitle has-text-centered">
         MAE: reconstruction of random masked images. We set the proportion of the random mask
to 50%.
        </h2>
      </div>
      <div class="item">
        <center>
        <img src="static/images/image8.png" alt="Bounds for inpainting"/>
        </center>
        <h2 class="subtitle has-text-centered">
          Outpainting by VQ-GAN. We retain 25% of the image center. 
        </h2>
      </div>
      <div class="item">
        <center>
        <img src="static/images/image9.png" alt="Comparing the different methods"/>
        </center>
        <h2 class="subtitle has-text-centered">
        Upward extension by VQ-GAN. We retain 50% of the lower half of the image.
       </h2>
     </div>
 <div class="item">
        <center>
        <img src="static/images/image10.png" alt="Bounds for inpainting"/>
        </center>
        <h2 class="subtitle has-text-centered">
         Leftward extension by VQ-GAN. We retain 50% of the right half of the image.
        </h2>
      </div>
<div class="item">
        <center>
        <img src="static/images/image11.png" alt="Bounds for inpainting"/>
        </center>
        <h2 class="subtitle has-text-centered">
        Downward extension by VQ-GAN. We crop the bottom 25% of the image.
        </h2>
      </div>
<div class="item">
        <center>
        <img src="static/images/image12.png" alt="Bounds for inpainting"/>
        </center>
        <h2 class="subtitle has-text-centered">
       Rightward extension by VQ-GAN. We crop the right 25% of the image.
        </h2>
      </div>
  </div>
</div>
</div>
</div>
</section>




<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
   <pre><code>@article{feng2024controllable,
  title={Controllable Unlearning for Image-to-Image Generative Models via $\epsilon$-Constrained Optimization},
  author={Feng, Xiaohua and Chen, Chaochao and Li, Yuyuan and Zhang, Li},
  journal={arXiv preprint arXiv:2408.01689},
  year={2024}
}</code></pre>
  </div>
</section>
<!--End BibTex citation -->


 <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/your_repo_here">Template repository</a>. The template is provided under the <a href="https://github.com/your_repo_here/blob/master/LICENSE">Creative Commons</a> license.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>



<!-- Statcounter tracking code -->

<!-- Default Statcounter code for SSRL AD
  http://www.vision.huji.ac.il/ssrl_ad/ -->
  <script type="text/javascript">
    var sc_project=12788650; 
    var sc_invisible=1; 
    var sc_security="cba63af3"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js"
  async></script>
  <noscript><div class="statcounter"><a title="Web Analytics"
    href="https://statcounter.com/" target="_blank"><img
    class="statcounter"
    src="https://c.statcounter.com/12788650/0/cba63af3/1/"
    alt="Web Analytics"
    referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
    <!-- End of Statcounter Code -->

    <!-- End of Statcounter Code -->

  </body>
  </html>

